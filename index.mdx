---
title: Introduction
description: Monitor, debug, and improve your AI agents with Foil
---

# Welcome to Foil

Foil is an observability platform purpose-built for AI applications. Get complete visibility into your LLM calls, agent workflows, and AI pipelines with distributed tracing, real-time alerting, and actionable analytics.

## Why Foil?

Building AI applications is hard. Debugging them is harder. Foil gives you the tools to understand what your AI is doing, why it's failing, and how to make it better.

<CardGroup cols={2}>
  <Card title="Distributed Tracing" icon="diagram-project">
    Track every LLM call, tool execution, and agent step with full context and parent-child relationships.
  </Card>
  <Card title="Real-time Alerting" icon="bell">
    Get notified instantly when your AI hallucinates, gets stuck, or produces low-quality outputs.
  </Card>
  <Card title="Cost Analytics" icon="chart-line">
    Monitor token usage, latency, and costs across all your models and agents.
  </Card>
  <Card title="Custom Signals" icon="signal">
    Track user feedback, quality scores, and custom metrics tied to your traces.
  </Card>
</CardGroup>

## How It Works

Choose the integration method that works best for you:

<Tabs>
  <Tab title="OpenTelemetry (Recommended)">
    Zero-code instrumentation with automatic tracing for OpenAI, Anthropic, LangChain, and more.

    <Steps>
      <Step title="Install the SDK">
        ```bash
        npm install @foil/foil-js
        ```
      </Step>
      <Step title="Initialize Foil">
        Add one line at the top of your app - all LLM calls are automatically traced.
        ```javascript
        const { Foil } = require('@foil/foil-js/otel');

        // Initialize Foil with OpenTelemetry
        Foil.init({
          apiKey: process.env.FOIL_API_KEY,
          agentName: 'my-agent',
        });

        // That's it! All OpenAI/Anthropic/LangChain calls are now traced
        const response = await openai.chat.completions.create({
          model: 'gpt-4',
          messages: [{ role: 'user', content: 'Hello!' }],
        });
        ```
      </Step>
      <Step title="View in Dashboard">
        See your traces, metrics, and alerts in the Foil dashboard.
      </Step>
    </Steps>
  </Tab>
  <Tab title="Native SDK">
    Full control with manual tracing for custom instrumentation.

    <Steps>
      <Step title="Install the SDK">
        ```bash
        npm install @foil/foil-js
        ```
      </Step>
      <Step title="Instrument Your Code">
        Wrap your AI calls with Foil's tracer to capture every interaction.
        ```javascript
        const { createFoilTracer, SpanKind } = require('@foil/foil-js');

        const tracer = createFoilTracer({
          apiKey: process.env.FOIL_API_KEY,
          agentName: 'my-agent',
        });

        await tracer.trace(async (ctx) => {
          const span = await ctx.startSpan(SpanKind.LLM, 'gpt-4', {
            input: messages,
          });

          const response = await openai.chat.completions.create({
            model: 'gpt-4',
            messages,
          });

          await span.end({
            output: response.choices[0].message.content,
            tokens: response.usage,
          });

          return response;
        }, { name: 'chat-completion' });
        ```
      </Step>
      <Step title="View in Dashboard">
        See your traces, metrics, and alerts in the Foil dashboard.
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Features

### Traces & Spans

Every AI interaction is captured as a **trace** containing multiple **spans**. Spans represent individual operations like LLM calls, tool executions, or retrieval steps. Foil automatically tracks:

- Input and output content
- Token usage (prompt, completion, total)
- Latency and time-to-first-token
- Errors and status codes
- Custom metadata

### Intelligent Alerting

Foil uses LLM analysis to detect issues that traditional monitoring misses:

**Content Evaluations:**
- **Hallucination Detection** - Identifies fabricated facts, fake entities, made-up citations
- **Quality Analysis** - Catches off-topic, unhelpful, or incoherent responses
- **Loop Detection** - Alerts when agents get stuck repeating themselves
- **Satisfaction Analysis** - Detects responses likely to leave users unsatisfied
- **Frustration Detection** - Identifies user frustration signals in conversations
- **Content Safety** - Flags inappropriate, explicit, or harmful content

**Security Evaluations:**
- **Prompt Injection** - Detects attempts to override instructions or extract system prompts
- **PII Leakage** - Identifies exposed personal data (SSN, credit cards, phone numbers)
- **Jailbreak Detection** - Catches bypass attempts like DAN, roleplay exploits

### Signals & Feedback

Capture user feedback and custom metrics directly tied to your traces:

- Thumbs up/down ratings
- Star ratings
- Sentiment analysis
- Goal completion tracking
- Custom metrics

## Quick Links

<CardGroup cols={2}>
  <Card title="Quickstart" icon="rocket" href="/quickstart">
    Get up and running in 5 minutes
  </Card>
  <Card title="OpenTelemetry" icon="circle-nodes" href="/sdks/javascript/opentelemetry">
    Zero-code auto-instrumentation
  </Card>
  <Card title="JavaScript SDK" icon="js" href="/sdks/javascript/installation">
    Full-featured SDK with tracing support
  </Card>
  <Card title="Python SDK" icon="python" href="/sdks/python/installation">
    Lightweight SDK for Python apps
  </Card>
</CardGroup>
